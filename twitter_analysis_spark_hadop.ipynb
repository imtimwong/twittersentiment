{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f01d2fe1908>\n",
      "+------------------------------------------------+\n",
      "|_c0                                             |\n",
      "+------------------------------------------------+\n",
      "|\"@flawarah \"\"How Haze Affects Milk Tea Brand.\"\"\"|\n",
      "|The haze are really bad                         |\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "                                                    _c0\n",
      "0      \"@flawarah \"\"How Haze Affects Milk Tea Brand.\"\"\"\n",
      "1                               The haze are really bad\n",
      "2     RT @allyxjackson: do yall understand just how ...\n",
      "3     RT @BRITgrlINDOfood: Standing on a bridge over...\n",
      "4        The haze is worse than when I arrived yesterâ€¦\"\n",
      "...                                                 ...\n",
      "4941                          @knowyouskz Sometime okay\n",
      "4942                   @techinsider POLLUTION REDUCTION\n",
      "4943  this is still the funniest shit ever esgdjgash...\n",
      "4944  The weather and the haze also makes me feel th...\n",
      "4945                  @techinsider  POLLUTION REDUCTION\n",
      "\n",
      "[4946 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # Call this only after findspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "print(spark)\n",
    "\n",
    "\n",
    "\n",
    "sparkSession = SparkSession.builder.appName(\"test_read_from_pg\").getOrCreate()\n",
    "\n",
    "\n",
    "data=\"hdfs://namenode:9000/user/test/input/tweets_pg_export.txt\"\n",
    "\n",
    "df_load = sparkSession.read.csv(data)\n",
    "df_load.show(2, False)\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "#conver Spark dataframe into Pandas dataframe\n",
    "df_pandas = df_load.select(\"*\").toPandas()\n",
    "\n",
    "print(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
